{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "9e575dc0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9e575dc0",
        "outputId": "c87d4081-4912-48f4-a30b-38ad6ecdb1dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Elevation    Aspect     Slope  Horizontal_Distance_To_Hydrology  \\\n",
            "0       -1.297805 -0.935157 -1.482820                         -0.053767   \n",
            "1       -1.319235 -0.890480 -1.616363                         -0.270188   \n",
            "2       -0.554907 -0.148836 -0.681563                         -0.006719   \n",
            "3       -0.622768 -0.005869  0.520322                         -0.129044   \n",
            "4       -1.301377 -0.988770 -1.616363                         -0.547771   \n",
            "...           ...       ...       ...                               ...   \n",
            "581007  -2.012130 -0.023740  0.787408                         -0.867697   \n",
            "581008  -2.029988 -0.032675  0.653865                         -0.952383   \n",
            "581009  -2.047847  0.029873  0.386780                         -0.985317   \n",
            "581010  -2.054990  0.128163  0.119694                         -0.985317   \n",
            "581011  -2.058562  0.083486 -0.147392                         -0.985317   \n",
            "\n",
            "        Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  \\\n",
            "0                            -0.796273                        -1.180146   \n",
            "1                            -0.899197                        -1.257106   \n",
            "2                             0.318742                         0.532212   \n",
            "3                             1.227908                         0.474492   \n",
            "4                            -0.813427                        -1.256464   \n",
            "...                                ...                              ...   \n",
            "581007                       -0.504653                        -1.437962   \n",
            "581008                       -0.590424                        -1.446299   \n",
            "581009                       -0.676194                        -1.449506   \n",
            "581010                       -0.710502                        -1.449506   \n",
            "581011                       -0.727656                        -1.464256   \n",
            "\n",
            "        Hillshade_9am  Hillshade_Noon  Hillshade_3pm  \\\n",
            "0            0.330743        0.439143       0.142960   \n",
            "1            0.293388        0.590899       0.221342   \n",
            "2            0.816364        0.742654      -0.196691   \n",
            "3            0.965786        0.742654      -0.536343   \n",
            "4            0.293388        0.540313       0.195215   \n",
            "...               ...             ...            ...   \n",
            "581007       1.040496        0.692069      -0.640851   \n",
            "581008       1.040496        0.692069      -0.614724   \n",
            "581009       0.891075        0.894409      -0.327327   \n",
            "581010       0.666942        1.096749       0.012325   \n",
            "581011       0.704298        1.046164      -0.039929   \n",
            "\n",
            "        Horizontal_Distance_To_Fire_Points  ...  Soil_Type32  Soil_Type33  \\\n",
            "0                                 3.246283  ...    -0.315238    -0.290284   \n",
            "1                                 3.205504  ...    -0.315238    -0.290284   \n",
            "2                                 3.126965  ...    -0.315238    -0.290284   \n",
            "3                                 3.194931  ...    -0.315238    -0.290284   \n",
            "4                                 3.165479  ...    -0.315238    -0.290284   \n",
            "...                                    ...  ...          ...          ...   \n",
            "581007                           -0.863386  ...    -0.315238    -0.290284   \n",
            "581008                           -0.857345  ...    -0.315238    -0.290284   \n",
            "581009                           -0.850548  ...    -0.315238    -0.290284   \n",
            "581010                           -0.842997  ...    -0.315238    -0.290284   \n",
            "581011                           -0.834690  ...    -0.315238    -0.290284   \n",
            "\n",
            "        Soil_Type34  Soil_Type35  Soil_Type36  Soil_Type37  Soil_Type38  \\\n",
            "0          -0.05273    -0.057143    -0.014313    -0.022653    -0.165956   \n",
            "1          -0.05273    -0.057143    -0.014313    -0.022653    -0.165956   \n",
            "2          -0.05273    -0.057143    -0.014313    -0.022653    -0.165956   \n",
            "3          -0.05273    -0.057143    -0.014313    -0.022653    -0.165956   \n",
            "4          -0.05273    -0.057143    -0.014313    -0.022653    -0.165956   \n",
            "...             ...          ...          ...          ...          ...   \n",
            "581007     -0.05273    -0.057143    -0.014313    -0.022653    -0.165956   \n",
            "581008     -0.05273    -0.057143    -0.014313    -0.022653    -0.165956   \n",
            "581009     -0.05273    -0.057143    -0.014313    -0.022653    -0.165956   \n",
            "581010     -0.05273    -0.057143    -0.014313    -0.022653    -0.165956   \n",
            "581011     -0.05273    -0.057143    -0.014313    -0.022653    -0.165956   \n",
            "\n",
            "        Soil_Type39  Soil_Type40  Cover_Type  \n",
            "0         -0.156014    -0.123654    2.111366  \n",
            "1         -0.156014    -0.123654    2.111366  \n",
            "2         -0.156014    -0.123654   -0.036857  \n",
            "3         -0.156014    -0.123654   -0.036857  \n",
            "4         -0.156014    -0.123654    2.111366  \n",
            "...             ...          ...         ...  \n",
            "581007    -0.156014    -0.123654    0.679218  \n",
            "581008    -0.156014    -0.123654    0.679218  \n",
            "581009    -0.156014    -0.123654    0.679218  \n",
            "581010    -0.156014    -0.123654    0.679218  \n",
            "581011    -0.156014    -0.123654    0.679218  \n",
            "\n",
            "[581012 rows x 55 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "class DataPreprocessor:\n",
        "    def __init__(self,\n",
        "                 use_one_hot_encoding=False,\n",
        "                 use_mean_imputation=False,\n",
        "                 use_mode_imputation=False,\n",
        "                 use_knn_imputation=False,\n",
        "                 use_standardization=False,\n",
        "                 use_min_max_scaling=False,\n",
        "                 use_noise_injection=False,\n",
        "                 noise_factor=0.01):\n",
        "\n",
        "        self.use_one_hot_encoding = use_one_hot_encoding\n",
        "        self.use_mean_imputation = use_mean_imputation\n",
        "        self.use_mode_imputation = use_mode_imputation\n",
        "        self.use_knn_imputation = use_knn_imputation\n",
        "        self.use_standardization = use_standardization\n",
        "        self.use_min_max_scaling = use_min_max_scaling\n",
        "        self.use_noise_injection = use_noise_injection\n",
        "        self.noise_factor = noise_factor\n",
        "\n",
        "        # Initialize transformers\n",
        "        self.ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)  # Fix here\n",
        "        self.imputer_mean = SimpleImputer(strategy='mean')\n",
        "        self.imputer_mode = SimpleImputer(strategy='most_frequent')\n",
        "        self.imputer_knn = KNeighborsClassifier(n_neighbors=5)\n",
        "        self.scaler_standard = StandardScaler()\n",
        "        self.scaler_minmax = MinMaxScaler()\n",
        "\n",
        "    def fit_transform(self, X):\n",
        "        # Automatically detect categorical and numerical columns\n",
        "        categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
        "        numerical_cols = X.select_dtypes(exclude=['object', 'category']).columns\n",
        "\n",
        "        # Handle categorical columns with One-Hot Encoding\n",
        "        if self.use_one_hot_encoding and len(categorical_cols) > 0:\n",
        "            X_cat = X[categorical_cols]\n",
        "            X_encoded = self.ohe.fit_transform(X_cat)\n",
        "            X_encoded_df = pd.DataFrame(X_encoded, columns=self.ohe.get_feature_names_out(categorical_cols))\n",
        "            X = X.drop(columns=categorical_cols)\n",
        "            X = pd.concat([X, X_encoded_df], axis=1)\n",
        "\n",
        "        # Handle Missing Data Imputation\n",
        "        if self.use_mean_imputation and len(numerical_cols) > 0:\n",
        "            X[numerical_cols] = self.imputer_mean.fit_transform(X[numerical_cols])\n",
        "        if self.use_mode_imputation and len(categorical_cols) > 0:\n",
        "            X[categorical_cols] = self.imputer_mode.fit_transform(X[categorical_cols])\n",
        "        if self.use_knn_imputation:\n",
        "            X = self.knn_imputation(X)\n",
        "\n",
        "        # Feature Scaling\n",
        "        if self.use_standardization and len(numerical_cols) > 0:\n",
        "            X[numerical_cols] = self.scaler_standard.fit_transform(X[numerical_cols])\n",
        "        if self.use_min_max_scaling and len(numerical_cols) > 0:\n",
        "            X[numerical_cols] = self.scaler_minmax.fit_transform(X[numerical_cols])\n",
        "\n",
        "        # Noise Injection for Robustness\n",
        "        if self.use_noise_injection:\n",
        "            X = self.inject_noise(X)\n",
        "\n",
        "        return X\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Automatically detect categorical and numerical columns\n",
        "        categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
        "        numerical_cols = X.select_dtypes(exclude=['object', 'category']).columns\n",
        "\n",
        "        # Handle categorical columns with One-Hot Encoding\n",
        "        if self.use_one_hot_encoding and len(categorical_cols) > 0:\n",
        "            X_cat = X[categorical_cols]\n",
        "            X_encoded = self.ohe.transform(X_cat)\n",
        "            X_encoded_df = pd.DataFrame(X_encoded, columns=self.ohe.get_feature_names_out(categorical_cols))\n",
        "            X = X.drop(columns=categorical_cols)\n",
        "            X = pd.concat([X, X_encoded_df], axis=1)\n",
        "\n",
        "        # Handle Missing Data Imputation\n",
        "        if self.use_mean_imputation and len(numerical_cols) > 0:\n",
        "            X[numerical_cols] = self.imputer_mean.transform(X[numerical_cols])\n",
        "        if self.use_mode_imputation and len(categorical_cols) > 0:\n",
        "            X[categorical_cols] = self.imputer_mode.transform(X[categorical_cols])\n",
        "        if self.use_knn_imputation:\n",
        "            X = self.knn_imputation(X)\n",
        "\n",
        "        # Feature Scaling\n",
        "        if self.use_standardization and len(numerical_cols) > 0:\n",
        "            X[numerical_cols] = self.scaler_standard.transform(X[numerical_cols])\n",
        "        if self.use_min_max_scaling and len(numerical_cols) > 0:\n",
        "            X[numerical_cols] = self.scaler_minmax.transform(X[numerical_cols])\n",
        "\n",
        "        # Noise Injection for Robustness\n",
        "        if self.use_noise_injection:\n",
        "            X = self.inject_noise(X)\n",
        "\n",
        "        return X\n",
        "\n",
        "    def knn_imputation(self, X):\n",
        "        # KNN Imputation for missing values\n",
        "        X_filled = X.copy()\n",
        "        for col in X.columns:\n",
        "            if X[col].isnull().any():\n",
        "                knn = self.imputer_knn.fit(X.dropna())\n",
        "                X_filled[col] = knn.predict(X[col].dropna().values.reshape(-1, 1))\n",
        "        return X_filled\n",
        "\n",
        "    def inject_noise(self, X):\n",
        "        # Inject random noise into the numerical features\n",
        "        noisy_X = X.copy()\n",
        "        numeric_cols = noisy_X.select_dtypes(include=[np.number]).columns\n",
        "        noise = np.random.normal(0, self.noise_factor, size=noisy_X[numeric_cols].shape)\n",
        "        noisy_X[numeric_cols] += noise\n",
        "        return noisy_X\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Generate some sample data\n",
        "    data = pd.read_csv(r'covtype.csv')\n",
        "\n",
        "    # Initialize preprocessor\n",
        "    preprocessor = DataPreprocessor(\n",
        "        use_one_hot_encoding=True,\n",
        "        use_mean_imputation=True,\n",
        "        use_standardization=True,\n",
        "        use_noise_injection=False,\n",
        "        noise_factor=0.01\n",
        "    )\n",
        "\n",
        "    # Preprocess the data\n",
        "    processed_data = preprocessor.fit_transform(data)\n",
        "    print(processed_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2688941a",
      "metadata": {
        "id": "2688941a",
        "outputId": "57c22677-6d34-4f81-aa8e-2c474f0c16a0"
      },
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "Target 7 is out of bounds.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[21], line 161\u001b[0m\n\u001b[0;32m    158\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoMLP(data\u001b[38;5;241m=\u001b[39mdata, target_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m, hidden_dims\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m32\u001b[39m], epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 161\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;66;03m# Evaluate the model (for classification: precision, recall, accuracy, F1 score)\u001b[39;00m\n\u001b[0;32m    164\u001b[0m predictions, true_labels \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate()\n",
            "Cell \u001b[1;32mIn[21], line 100\u001b[0m, in \u001b[0;36mAutoMLP.train_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m     99\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(X_train_tensor)\n\u001b[1;32m--> 100\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Remove extra dimension\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[0;32m    103\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1188\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1189\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1190\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:3104\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3103\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mIndexError\u001b[0m: Target 7 is out of bounds."
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score, f1_score, mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Download dataset (replace with the actual dataset path on your system)\n",
        "# For example:\n",
        "# data = pd.read_csv('path_to_forest_cover_type.csv')\n",
        "\n",
        "# Assuming the data is already loaded as 'data'\n",
        "\n",
        "class AutoMLP:\n",
        "    def __init__(self, data, target_col, hidden_dims=[64, 32], batch_size=64, epochs=100, learning_rate=0.001, noise_factor=0.01):\n",
        "        \"\"\"\n",
        "        Initialize the AutoMLP class for automatic classification or regression task.\n",
        "\n",
        "        :param data: pandas DataFrame, contains the input features and target column\n",
        "        :param target_col: str, name of the target column\n",
        "        :param hidden_dims: list of int, sizes of the hidden layers\n",
        "        :param batch_size: int, batch size for training\n",
        "        :param epochs: int, number of training epochs\n",
        "        :param learning_rate: float, learning rate for the optimizer\n",
        "        :param noise_factor: float, factor for noise injection\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.target_col = target_col\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "        self.learning_rate = learning_rate\n",
        "        self.noise_factor = noise_factor\n",
        "\n",
        "        # Separate features and target\n",
        "        self.X = self.data.drop(columns=[self.target_col])\n",
        "        self.y = self.data[self.target_col]\n",
        "\n",
        "        # Detect task type\n",
        "        self.is_classification = self._detect_task_type()\n",
        "        self.output_dim = len(np.unique(self.y)) if self.is_classification else 1\n",
        "\n",
        "        # Train-test split\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Create model\n",
        "        self.model = self._build_model()\n",
        "\n",
        "    def _detect_task_type(self):\n",
        "        \"\"\"Detect if the task is classification or regression based on the target column.\"\"\"\n",
        "        if len(np.unique(self.y)) <= 10 and np.all(self.y.apply(lambda x: isinstance(x, (int, np.integer)))):\n",
        "            return True  # Classification (less than or equal to 10 unique values, integers)\n",
        "        return False  # Regression\n",
        "\n",
        "    def _build_model(self):\n",
        "        \"\"\"Build the MLP model.\"\"\"\n",
        "        input_dim = self.X.shape[1]\n",
        "\n",
        "        layers = []\n",
        "        layers.append(nn.Linear(input_dim, self.hidden_dims[0]))\n",
        "        layers.append(nn.ReLU())\n",
        "\n",
        "        # Hidden layers\n",
        "        for i in range(1, len(self.hidden_dims)):\n",
        "            layers.append(nn.Linear(self.hidden_dims[i-1], self.hidden_dims[i]))\n",
        "            layers.append(nn.ReLU())\n",
        "\n",
        "        # Output layer\n",
        "        layers.append(nn.Linear(self.hidden_dims[-1], self.output_dim))\n",
        "\n",
        "        # For classification, apply Softmax activation on the output layer\n",
        "        if self.is_classification:\n",
        "            self.final_activation = nn.Softmax(dim=1)\n",
        "        else:\n",
        "            self.final_activation = nn.Identity()\n",
        "\n",
        "        model = nn.Sequential(*layers)\n",
        "        return model\n",
        "\n",
        "    def train_model(self):\n",
        "        \"\"\"Train the model.\"\"\"\n",
        "        # Convert to PyTorch tensors\n",
        "        X_train_tensor = torch.tensor(self.X_train.values, dtype=torch.float32)\n",
        "        y_train_tensor = torch.tensor(self.y_train.values, dtype=torch.long if self.is_classification else torch.float32).view(-1, 1)\n",
        "\n",
        "        # Loss function and optimizer\n",
        "        loss_function = nn.CrossEntropyLoss() if self.is_classification else nn.MSELoss()\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(self.epochs):\n",
        "            self.model.train()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = self.model(X_train_tensor)\n",
        "            loss = loss_function(outputs, y_train_tensor.squeeze())  # Remove extra dimension\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                print(f\"Epoch [{epoch+1}/{self.epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    def evaluate(self):\n",
        "        \"\"\"Evaluate the model and return metrics.\"\"\"\n",
        "        # Convert to PyTorch tensors\n",
        "        X_test_tensor = torch.tensor(self.X_test.values, dtype=torch.float32)\n",
        "        y_test_tensor = torch.tensor(self.y_test.values, dtype=torch.long if self.is_classification else torch.float32).view(-1, 1)\n",
        "\n",
        "        # Evaluate the model\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(X_test_tensor)\n",
        "            outputs = self.final_activation(outputs)  # Apply final activation (Softmax for classification)\n",
        "\n",
        "            # For classification, use argmax to get predicted classes\n",
        "            if self.is_classification:\n",
        "                predicted = torch.argmax(outputs, dim=1)\n",
        "                self._print_classification_metrics(predicted, y_test_tensor)\n",
        "            else:\n",
        "                predicted = outputs\n",
        "                self._print_regression_metrics(predicted, y_test_tensor)\n",
        "\n",
        "            return predicted.numpy(), y_test_tensor.numpy()\n",
        "\n",
        "    def _print_classification_metrics(self, predicted, true):\n",
        "        \"\"\"Print classification metrics (accuracy, precision, recall, F1 score).\"\"\"\n",
        "        accuracy = accuracy_score(true, predicted)\n",
        "        precision = precision_score(true, predicted, average='weighted', zero_division=0)\n",
        "        recall = recall_score(true, predicted, average='weighted', zero_division=0)\n",
        "        f1 = f1_score(true, predicted, average='weighted', zero_division=0)\n",
        "\n",
        "        print(f\"Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"Precision: {precision:.4f}\")\n",
        "        print(f\"Recall: {recall:.4f}\")\n",
        "        print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "    def _print_regression_metrics(self, predicted, true):\n",
        "        \"\"\"Print regression metrics (Mean Squared Error).\"\"\"\n",
        "        mse = mean_squared_error(true, predicted)\n",
        "        print(f\"Mean Squared Error: {mse:.4f}\")\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Load the dataset (replace with the actual dataset path or Kaggle API call)\n",
        "    # For demonstration, using a dataset like 'load_iris' here\n",
        "\n",
        "    data = pd.read_csv(r'C:\\Users\\Shaikh\\Documents\\SRP\\covtype.csv')\n",
        "\n",
        "\n",
        "    # Initialize preprocessor\n",
        "    preprocessor = DataPreprocessor(\n",
        "        use_one_hot_encoding=True,\n",
        "        use_mean_imputation=True,\n",
        "        use_standardization=True,\n",
        "        use_noise_injection=False,\n",
        "        noise_factor=0.01\n",
        "    )\n",
        "\n",
        "    # Preprocess the data\n",
        "    X_processed = preprocessor.fit_transform(data.drop(columns=['target']))\n",
        "    data['target'] = data['Cover_Type']  # This should be the target column\n",
        "\n",
        "    # Initialize AutoMLP (this will automatically detect the task type)\n",
        "    model = AutoMLP(data=data, target_col='target', hidden_dims=[64, 32], epochs=100)\n",
        "\n",
        "    # Train the model\n",
        "    model.train_model()\n",
        "\n",
        "    # Evaluate the model (for classification: precision, recall, accuracy, F1 score)\n",
        "    predictions, true_labels = model.evaluate()\n",
        "    print(\"Predictions:\", predictions)\n",
        "    print(\"True labels:\", true_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "698f0b50",
      "metadata": {
        "id": "698f0b50"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, mean_squared_error\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# DataPreprocessor Class\n",
        "class DataPreprocessor:\n",
        "    def __init__(self,\n",
        "                 use_one_hot_encoding=False,\n",
        "                 use_mean_imputation=False,\n",
        "                 use_mode_imputation=False,\n",
        "                 use_knn_imputation=False,\n",
        "                 use_standardization=False,\n",
        "                 use_min_max_scaling=False,\n",
        "                 use_noise_injection=False,\n",
        "                 noise_factor=0.01):\n",
        "\n",
        "        self.use_one_hot_encoding = use_one_hot_encoding\n",
        "        self.use_mean_imputation = use_mean_imputation\n",
        "        self.use_mode_imputation = use_mode_imputation\n",
        "        self.use_knn_imputation = use_knn_imputation\n",
        "        self.use_standardization = use_standardization\n",
        "        self.use_min_max_scaling = use_min_max_scaling\n",
        "        self.use_noise_injection = use_noise_injection\n",
        "        self.noise_factor = noise_factor\n",
        "\n",
        "        self.ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "        self.imputer_mean = SimpleImputer(strategy='mean')\n",
        "        self.imputer_mode = SimpleImputer(strategy='most_frequent')\n",
        "        self.imputer_knn = KNeighborsClassifier(n_neighbors=5)\n",
        "        self.scaler_standard = StandardScaler()\n",
        "        self.scaler_minmax = MinMaxScaler()\n",
        "\n",
        "    def fit_transform(self, X):\n",
        "        categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
        "        numerical_cols = X.select_dtypes(exclude=['object', 'category']).columns\n",
        "\n",
        "        # Imputation\n",
        "        if self.use_mean_imputation and len(numerical_cols) > 0:\n",
        "            X[numerical_cols] = self.imputer_mean.fit_transform(X[numerical_cols])\n",
        "        if self.use_mode_imputation and len(categorical_cols) > 0:\n",
        "            X[categorical_cols] = self.imputer_mode.fit_transform(X[categorical_cols])\n",
        "\n",
        "        # One-hot encoding\n",
        "        if self.use_one_hot_encoding and len(categorical_cols) > 0:\n",
        "            X_cat = X[categorical_cols]\n",
        "            X_encoded = self.ohe.fit_transform(X_cat)\n",
        "            X_encoded_df = pd.DataFrame(X_encoded, columns=self.ohe.get_feature_names_out(categorical_cols), index=X.index)\n",
        "            X = X.drop(columns=categorical_cols)\n",
        "            X = pd.concat([X, X_encoded_df], axis=1)\n",
        "\n",
        "        # Scaling\n",
        "        if self.use_standardization and len(numerical_cols) > 0:\n",
        "            X[numerical_cols] = self.scaler_standard.fit_transform(X[numerical_cols])\n",
        "        elif self.use_min_max_scaling and len(numerical_cols) > 0:\n",
        "            X[numerical_cols] = self.scaler_minmax.fit_transform(X[numerical_cols])\n",
        "\n",
        "        # Noise injection\n",
        "        if self.use_noise_injection:\n",
        "            X = self.inject_noise(X)\n",
        "\n",
        "        return X\n",
        "\n",
        "    def transform(self, X):\n",
        "        categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
        "        numerical_cols = X.select_dtypes(exclude=['object', 'category']).columns\n",
        "\n",
        "        if self.use_mean_imputation and len(numerical_cols) > 0:\n",
        "            X[numerical_cols] = self.imputer_mean.transform(X[numerical_cols])\n",
        "        if self.use_mode_imputation and len(categorical_cols) > 0:\n",
        "            X[categorical_cols] = self.imputer_mode.transform(X[categorical_cols])\n",
        "\n",
        "        if self.use_one_hot_encoding and len(categorical_cols) > 0:\n",
        "            X_cat = X[categorical_cols]\n",
        "            X_encoded = self.ohe.transform(X_cat)\n",
        "            X_encoded_df = pd.DataFrame(X_encoded, columns=self.ohe.get_feature_names_out(categorical_cols), index=X.index)\n",
        "            X = X.drop(columns=categorical_cols)\n",
        "            X = pd.concat([X, X_encoded_df], axis=1)\n",
        "\n",
        "        if self.use_standardization and len(numerical_cols) > 0:\n",
        "            X[numerical_cols] = self.scaler_standard.transform(X[numerical_cols])\n",
        "        elif self.use_min_max_scaling and len(numerical_cols) > 0:\n",
        "            X[numerical_cols] = self.scaler_minmax.transform(X[numerical_cols])\n",
        "\n",
        "        if self.use_noise_injection:\n",
        "            X = self.inject_noise(X)\n",
        "\n",
        "        return X\n",
        "\n",
        "    def inject_noise(self, X):\n",
        "        noisy_X = X.copy()\n",
        "        numeric_cols = noisy_X.select_dtypes(include=[np.number]).columns\n",
        "        noise = np.random.normal(0, self.noise_factor, size=noisy_X[numeric_cols].shape)\n",
        "        noisy_X[numeric_cols] += noise\n",
        "        return noisy_X\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "e6f79a8e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6f79a8e",
        "outputId": "63dfcf98-117e-4a41-8148-58b055e7fb33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 1.8654\n",
            "Epoch [20/100], Loss: 1.6754\n",
            "Epoch [30/100], Loss: 1.4160\n",
            "Epoch [40/100], Loss: 1.1675\n",
            "Epoch [50/100], Loss: 1.0170\n",
            "Epoch [60/100], Loss: 0.9331\n",
            "Epoch [70/100], Loss: 0.8703\n",
            "Epoch [80/100], Loss: 0.8234\n",
            "Epoch [90/100], Loss: 0.7866\n",
            "Epoch [100/100], Loss: 0.7585\n",
            "Classification Metrics:\n",
            "Accuracy: 0.6824092321196527\n",
            "Precision: 0.6810717790464706\n",
            "Recall: 0.6824092321196527\n",
            "F1 Score: 0.6550432185817675\n",
            "Predictions: [0 1 1 ... 1 1 6]\n",
            "True labels: [0 1 1 ... 1 1 6]\n"
          ]
        }
      ],
      "source": [
        "# AutoMLP Class\n",
        "class AutoMLP:\n",
        "    def __init__(self, X, y, hidden_dims=[64, 32], batch_size=64, epochs=140, learning_rate=0.001):\n",
        "        \"\"\"\n",
        "        :param X: preprocessed pandas DataFrame of features (all numeric)\n",
        "        :param y: pandas Series or numpy array of target values (classification or regression)\n",
        "        \"\"\"\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.is_classification = self._detect_task_type()\n",
        "        self.output_dim = len(np.unique(self.y)) if self.is_classification else 1\n",
        "\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
        "            self.X, self.y, test_size=0.2, random_state=42)\n",
        "\n",
        "        self.model = self._build_model()\n",
        "\n",
        "    def _detect_task_type(self):\n",
        "        if len(np.unique(self.y)) <= 10 and np.all(np.mod(self.y, 1) == 0):\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def _build_model(self):\n",
        "        input_dim = self.X.shape[1]\n",
        "        layers = []\n",
        "        layers.append(nn.Linear(input_dim, self.hidden_dims[0]))\n",
        "        layers.append(nn.ReLU())\n",
        "        for i in range(1, len(self.hidden_dims)):\n",
        "            layers.append(nn.Linear(self.hidden_dims[i-1], self.hidden_dims[i]))\n",
        "            layers.append(nn.ReLU())\n",
        "        layers.append(nn.Linear(self.hidden_dims[-1], self.output_dim))\n",
        "        if self.is_classification:\n",
        "            self.final_activation = nn.Softmax(dim=1)\n",
        "        else:\n",
        "            self.final_activation = nn.Identity()\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def train_model(self):\n",
        "        X_train_tensor = torch.tensor(self.X_train.values, dtype=torch.float32)\n",
        "        y_train_tensor = torch.tensor(self.y_train.values, dtype=torch.long if self.is_classification else torch.float32)\n",
        "\n",
        "        loss_fn = nn.CrossEntropyLoss() if self.is_classification else nn.MSELoss()\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            self.model.train()\n",
        "            optimizer.zero_grad()\n",
        "            outputs = self.model(X_train_tensor)\n",
        "            if self.is_classification:\n",
        "                loss = loss_fn(outputs, y_train_tensor)\n",
        "            else:\n",
        "                loss = loss_fn(outputs.squeeze(), y_train_tensor)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                print(f\"Epoch [{epoch+1}/{self.epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    def evaluate(self):\n",
        "        X_test_tensor = torch.tensor(self.X_test.values, dtype=torch.float32)\n",
        "        y_test_tensor = torch.tensor(self.y_test.values, dtype=torch.long if self.is_classification else torch.float32)\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(X_test_tensor)\n",
        "            outputs = self.final_activation(outputs)\n",
        "            if self.is_classification:\n",
        "                preds = torch.argmax(outputs, dim=1).numpy()\n",
        "                self._print_classification_metrics(preds, y_test_tensor.numpy())\n",
        "            else:\n",
        "                preds = outputs.squeeze().numpy()\n",
        "                self._print_regression_metrics(preds, y_test_tensor.numpy())\n",
        "            return preds, y_test_tensor.numpy()\n",
        "\n",
        "    def _print_classification_metrics(self, preds, true):\n",
        "        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "        print(\"Classification Metrics:\")\n",
        "        print(\"Accuracy:\", accuracy_score(true, preds))\n",
        "        print(\"Precision:\", precision_score(true, preds, average='weighted', zero_division=0))\n",
        "        print(\"Recall:\", recall_score(true, preds, average='weighted', zero_division=0))\n",
        "        print(\"F1 Score:\", f1_score(true, preds, average='weighted', zero_division=0))\n",
        "\n",
        "    def _print_regression_metrics(self, preds, true):\n",
        "        from sklearn.metrics import mean_squared_error\n",
        "        mse = mean_squared_error(true, preds)\n",
        "        print(\"Regression Metrics:\")\n",
        "        print(\"Mean Squared Error:\", mse)\n",
        "\n",
        "\n",
        "# Usage example\n",
        "if __name__ == \"__main__\":\n",
        "    data = pd.read_csv(r'covtype.csv')\n",
        "\n",
        "    target_col = 'Cover_Type'\n",
        "\n",
        "    # Separate features and target\n",
        "    X = data.drop(columns=[target_col])\n",
        "    y = data[target_col]\n",
        "\n",
        "    # Re-index target labels for PyTorch\n",
        "    y = y - y.min()\n",
        "\n",
        "    # Preprocess features only\n",
        "    preprocessor = DataPreprocessor(\n",
        "        use_one_hot_encoding=True,\n",
        "        use_mean_imputation=True,\n",
        "        use_standardization=True,\n",
        "        use_noise_injection=False,\n",
        "        noise_factor=0.01\n",
        "    )\n",
        "    X_processed = preprocessor.fit_transform(X)\n",
        "\n",
        "    # Train and evaluate\n",
        "    model = AutoMLP(X_processed, y, hidden_dims=[64, 32], epochs=100)\n",
        "    model.train_model()\n",
        "    preds, true = model.evaluate()\n",
        "\n",
        "    print(\"Predictions:\", preds)\n",
        "    print(\"True labels:\", true)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLh5UUPyq51G",
        "outputId": "8b34a606-1f84-4193-aff7-00447e088de2"
      },
      "id": "ZLh5UUPyq51G",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "116203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(X_processed))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okosW8APkBum",
        "outputId": "f94cdbe6-1c86-4e35-d398-76e55d44ce73"
      },
      "id": "okosW8APkBum",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "581012\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# step1_multi_mlp_embeddings.py\n",
        "import os, json, math, numpy as np, pandas as pd\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple, List, Optional, Dict\n",
        "\n",
        "# ── sklearn: robust, generic preprocessing ──────────────────────────────────────\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
        "\n",
        "# ── torch: shallow MLPs + training ─────────────────────────────────────────────\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# ----------------------------- CONFIG ------------------------------------------\n",
        "@dataclass\n",
        "class PrepConfig:\n",
        "    impute_num: str = \"mean\"          # \"mean\" | \"median\" | \"knn\"\n",
        "    impute_cat: str = \"most_frequent\" # \"most_frequent\" | \"constant\"\n",
        "    scale: Optional[str] = \"standard\" # None | \"standard\" | \"minmax\"\n",
        "    noise_std: float = 0.0            # e.g., 0.01 for light noise on numeric features\n",
        "    knn_k: int = 5\n",
        "\n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "    n_mlps: int = 5\n",
        "    hidden_dim: int = 128\n",
        "    n_hidden_layers: int = 3          # shallow by default\n",
        "    dropout: float = 0.1\n",
        "    batch_size: int = 512\n",
        "    epochs: int = 35\n",
        "    lr: float = 1e-3\n",
        "    weight_decay: float = 1e-5\n",
        "    val_size: float = 0.2\n",
        "    test_size: float = 0.2\n",
        "    seed: int = 42\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    # early stopping\n",
        "    patience: int = 5\n",
        "\n",
        "# ----------------------- PREPROCESSOR (generic) --------------------------------\n",
        "class TabularPreprocessor:\n",
        "    def __init__(self, cfg: PrepConfig):\n",
        "        self.cfg = cfg\n",
        "        self.column_transformer: Optional[ColumnTransformer] = None\n",
        "        self.feature_names_: Optional[List[str]] = None\n",
        "        self.num_cols_: Optional[List[str]] = None\n",
        "        self.cat_cols_: Optional[List[str]] = None\n",
        "\n",
        "    def _num_imputer(self):\n",
        "        if self.cfg.impute_num == \"mean\":\n",
        "            return SimpleImputer(strategy=\"mean\")\n",
        "        if self.cfg.impute_num == \"median\":\n",
        "            return SimpleImputer(strategy=\"median\")\n",
        "        if self.cfg.impute_num == \"knn\":\n",
        "            return KNNImputer(n_neighbors=self.cfg.knn_k)\n",
        "        raise ValueError(\"impute_num must be mean|median|knn\")\n",
        "\n",
        "    def _scaler(self):\n",
        "        if self.cfg.scale is None:\n",
        "            return \"passthrough\"\n",
        "        if self.cfg.scale == \"standard\":\n",
        "            return StandardScaler()\n",
        "        if self.cfg.scale == \"minmax\":\n",
        "            return MinMaxScaler()\n",
        "        raise ValueError(\"scale must be None|standard|minmax\")\n",
        "\n",
        "    def fit(self, X: pd.DataFrame):\n",
        "        # Treat bools as categorical (common in tabular datasets)\n",
        "        self.cat_cols_ = X.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
        "        self.num_cols_ = X.columns.difference(self.cat_cols_).tolist()\n",
        "\n",
        "        num_pipe = Pipeline(steps=[\n",
        "            (\"imputer\", self._num_imputer()),\n",
        "            (\"scaler\", self._scaler()),\n",
        "        ])\n",
        "\n",
        "        cat_pipe = Pipeline(steps=[\n",
        "            (\"imputer\", SimpleImputer(strategy=self.cfg.impute_cat, fill_value=\"missing\")),\n",
        "            (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
        "        ])\n",
        "\n",
        "        self.column_transformer = ColumnTransformer(\n",
        "            transformers=[\n",
        "                (\"num\", num_pipe, self.num_cols_),\n",
        "                (\"cat\", cat_pipe, self.cat_cols_)\n",
        "            ],\n",
        "            remainder=\"drop\",\n",
        "        )\n",
        "        self.column_transformer.fit(X)\n",
        "\n",
        "        # Build feature names after fit\n",
        "        names = []\n",
        "        if self.num_cols_:\n",
        "            names += self.num_cols_\n",
        "        if self.cat_cols_:\n",
        "            ohe = self.column_transformer.named_transformers_[\"cat\"][\"ohe\"]\n",
        "            names += ohe.get_feature_names_out(self.cat_cols_).tolist()\n",
        "        self.feature_names_ = names\n",
        "        return self\n",
        "\n",
        "    def transform(self, X: pd.DataFrame) -> np.ndarray:\n",
        "        Xt = self.column_transformer.transform(X)\n",
        "        Xt = np.asarray(Xt, dtype=np.float32)\n",
        "        # optional light Gaussian noise on numeric part (first len(num_cols_) columns after pipeline)\n",
        "        if self.cfg.noise_std and self.cfg.noise_std > 0 and len(self.num_cols_) > 0:\n",
        "            n_num = len(self.num_cols_)\n",
        "            noise = np.random.normal(0, self.cfg.noise_std, size=(Xt.shape[0], n_num)).astype(np.float32)\n",
        "            Xt[:, :n_num] += noise\n",
        "        return Xt\n",
        "\n",
        "# ----------------------------- TORCH DATASET -----------------------------------\n",
        "class TabularDataset(Dataset):\n",
        "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
        "        self.X = torch.from_numpy(X.astype(np.float32))\n",
        "        self.y = torch.from_numpy(y)\n",
        "\n",
        "    def __len__(self): return self.X.shape[0]\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "\n",
        "# ----------------------------- MODEL -------------------------------------------\n",
        "class ShallowMLP(nn.Module):\n",
        "    def __init__(self, in_dim: int, hidden: int, n_hidden_layers: int, dropout: float, out_dim: int, task: str):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        dim = in_dim\n",
        "        for _ in range(n_hidden_layers):\n",
        "            layers += [nn.Linear(dim, hidden), nn.ReLU(), nn.Dropout(dropout)]\n",
        "            dim = hidden\n",
        "        self.backbone = nn.Sequential(*layers) if layers else nn.Identity()\n",
        "        self.head = nn.Linear(dim, out_dim)\n",
        "        self.task = task  # \"binary\" | \"multiclass\" | \"regression\"\n",
        "\n",
        "    def forward(self, x, return_embedding: bool = False):\n",
        "        emb = self.backbone(x)\n",
        "        logits = self.head(emb)\n",
        "        if return_embedding:\n",
        "            return logits, emb\n",
        "        return logits\n",
        "\n",
        "# ----------------------------- UTIL --------------------------------------------\n",
        "def infer_task_and_outdim(y: np.ndarray) -> Tuple[str, int, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Returns (task, out_dim, y_torch_ready)\n",
        "    \"\"\"\n",
        "    if y.dtype.kind in {\"f\"} and len(np.unique(y)) > 10:\n",
        "        return \"regression\", 1, y.astype(np.float32).reshape(-1, 1)\n",
        "    # classification\n",
        "    classes = np.unique(y)\n",
        "    if len(classes) == 2:\n",
        "        # map to {0,1}\n",
        "        mapping = {classes[0]: 0, classes[1]: 1}\n",
        "        y_ = np.vectorize(mapping.get)(y).astype(np.int64)\n",
        "        return \"binary\", 1, y_.reshape(-1)\n",
        "    else:\n",
        "        # 0..K-1\n",
        "        mapping = {c: i for i, c in enumerate(classes)}\n",
        "        y_ = np.vectorize(mapping.get)(y).astype(np.int64)\n",
        "        return \"multiclass\", len(classes), y_.reshape(-1)\n",
        "\n",
        "def make_loaders(Xtr, ytr, Xva, yva, Xte, yte, bs):\n",
        "    ds_tr, ds_va, ds_te = TabularDataset(Xtr, ytr), TabularDataset(Xva, yva), TabularDataset(Xte, yte)\n",
        "    return (DataLoader(ds_tr, batch_size=bs, shuffle=True),\n",
        "            DataLoader(ds_va, batch_size=bs, shuffle=False),\n",
        "            DataLoader(ds_te, batch_size=bs, shuffle=False))\n",
        "\n",
        "def make_loss(task: str):\n",
        "    if task == \"binary\":     return nn.BCEWithLogitsLoss()\n",
        "    if task == \"multiclass\": return nn.CrossEntropyLoss()\n",
        "    return nn.MSELoss()  # regression\n",
        "\n",
        "def metric_from_logits(task: str, logits: torch.Tensor, y: torch.Tensor) -> float:\n",
        "    if task == \"binary\":\n",
        "        preds = (torch.sigmoid(logits).view(-1) > 0.5).long()\n",
        "        return (preds == y.long()).float().mean().item()\n",
        "    if task == \"multiclass\":\n",
        "        preds = logits.argmax(dim=1)\n",
        "        return (preds == y.long()).float().mean().item()\n",
        "    # regression -> negative RMSE so higher is better for early stopping\n",
        "    mse = nn.functional.mse_loss(logits.view_as(y).float(), y.float()).item()\n",
        "    return -math.sqrt(mse)\n",
        "\n",
        "# ----------------------------- TRAIN ONE MLP -----------------------------------\n",
        "def train_one_mlp(model, loaders, cfg: TrainConfig, task: str):\n",
        "    tr_loader, va_loader, _ = loaders\n",
        "    device = cfg.device\n",
        "    model.to(device)\n",
        "    opt = optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "    loss_fn = make_loss(task)\n",
        "    best_score, best_state, patience = -1e9, None, cfg.patience\n",
        "\n",
        "    for epoch in range(cfg.epochs):\n",
        "        model.train()\n",
        "        for xb, yb in tr_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            opt.zero_grad()\n",
        "            logits = model(xb)\n",
        "            if task == \"binary\":\n",
        "                loss = loss_fn(logits.view(-1), yb.float())\n",
        "            elif task == \"multiclass\":\n",
        "                loss = loss_fn(logits, yb.long())\n",
        "            else:\n",
        "                loss = loss_fn(logits, yb.float())\n",
        "            loss.backward(); opt.step()\n",
        "\n",
        "        # validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            scores = []\n",
        "            for xb, yb in va_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                logits = model(xb)\n",
        "                scores.append(metric_from_logits(task, logits, yb))\n",
        "            score = float(np.mean(scores)) if scores else -1e9\n",
        "\n",
        "        if score > best_score + 1e-6:\n",
        "            best_score, best_state, patience = score, {k: v.cpu().clone() for k, v in model.state_dict().items()}, cfg.patience\n",
        "        else:\n",
        "            patience -= 1\n",
        "            if patience <= 0:\n",
        "                break\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "    return model, best_score\n",
        "\n",
        "# ----------------------------- EMBEDDING DUMP ----------------------------------\n",
        "@torch.no_grad()\n",
        "def dump_embeddings(models: List[ShallowMLP], loader: DataLoader, task: str, device: str) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Returns array (N_examples, n_mlps, emb_dim)\n",
        "    \"\"\"\n",
        "    # probe emb dim\n",
        "    for m in models: m.eval().to(device)\n",
        "    # run one batch to find embedding dimension\n",
        "    xb0, _ = next(iter(loader))\n",
        "    xb0 = xb0.to(device)\n",
        "    _, emb0 = models[0](xb0, return_embedding=True)\n",
        "    emb_dim = emb0.shape[1]\n",
        "    # collect\n",
        "    all_embs = [ [] for _ in models ]\n",
        "    for xb, _ in loader:\n",
        "        xb = xb.to(device)\n",
        "        for i, m in enumerate(models):\n",
        "            _, emb = m(xb, return_embedding=True)\n",
        "            all_embs[i].append(emb.cpu().numpy())\n",
        "    stacked = [np.concatenate(chunks, axis=0) for chunks in all_embs]  # list of (N, emb_dim)\n",
        "    return np.stack(stacked, axis=1)  # (N, n_mlps, emb_dim)\n",
        "\n",
        "# ----------------------------- MAIN ENTRY --------------------------------------\n",
        "def train_mlps_and_save_embeddings(\n",
        "    df: pd.DataFrame,\n",
        "    target_col: str,\n",
        "    out_dir: str = \"./artifacts_step1\",\n",
        "    prep_cfg: PrepConfig = PrepConfig(),\n",
        "    train_cfg: TrainConfig = TrainConfig(),\n",
        "):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    rng = np.random.RandomState(train_cfg.seed)\n",
        "\n",
        "    y_raw = df[target_col].values\n",
        "    X_raw = df.drop(columns=[target_col])\n",
        "    print('hello1')\n",
        "    # split (stratify if classification)\n",
        "    task0, _, _ = infer_task_and_outdim(y_raw)\n",
        "    if task0 in (\"binary\", \"multiclass\"):\n",
        "        strat = y_raw\n",
        "        splitter = StratifiedShuffleSplit(n_splits=1, test_size=train_cfg.test_size, random_state=train_cfg.seed)\n",
        "        train_idx, test_idx = next(splitter.split(X_raw, strat))\n",
        "    else:\n",
        "        idx = np.arange(len(df)); rng.shuffle(idx)\n",
        "        split = int(len(idx) * (1 - train_cfg.test_size))\n",
        "        train_idx, test_idx = idx[:split], idx[split:]\n",
        "\n",
        "    print('hello2')\n",
        "    X_train, X_test = X_raw.iloc[train_idx].copy(), X_raw.iloc[test_idx].copy()\n",
        "    y_train_raw, y_test_raw = y_raw[train_idx], y_raw[test_idx]\n",
        "\n",
        "    # build preprocessor on train, transform all\n",
        "    prep = TabularPreprocessor(prep_cfg).fit(X_train)\n",
        "    X_train_t = prep.transform(X_train)\n",
        "    X_test_t  = prep.transform(X_test)\n",
        "\n",
        "    # secondary split: train/val\n",
        "    if task0 in (\"binary\", \"multiclass\"):\n",
        "        trX, vaX, trY_raw, vaY_raw = train_test_split(\n",
        "            X_train_t, y_train_raw, test_size=train_cfg.val_size, random_state=train_cfg.seed, stratify=y_train_raw\n",
        "        )\n",
        "    else:\n",
        "        trX, vaX, trY_raw, vaY_raw = train_test_split(\n",
        "            X_train_t, y_train_raw, test_size=train_cfg.val_size, random_state=train_cfg.seed\n",
        "        )\n",
        "\n",
        "    # infer final task using training labels only\n",
        "    task, out_dim, trY = infer_task_and_outdim(trY_raw)\n",
        "    _, _, vaY = infer_task_and_outdim(vaY_raw)\n",
        "    _, _, teY = infer_task_and_outdim(y_test_raw)\n",
        "\n",
        "    tr_loader, va_loader, te_loader = make_loaders(trX, trY, vaX, vaY, X_test_t, teY, train_cfg.batch_size)\n",
        "\n",
        "    in_dim = trX.shape[1]\n",
        "    models: List[ShallowMLP] = []\n",
        "    val_scores: List[float] = []\n",
        "\n",
        "    for i in range(train_cfg.n_mlps):\n",
        "        torch.manual_seed(train_cfg.seed + i)\n",
        "        mlp = ShallowMLP(\n",
        "            in_dim=in_dim,\n",
        "            hidden=train_cfg.hidden_dim,\n",
        "            n_hidden_layers=train_cfg.n_hidden_layers,\n",
        "            dropout=train_cfg.dropout,\n",
        "            out_dim=out_dim,\n",
        "            task=task\n",
        "        )\n",
        "        mlp, score = train_one_mlp(mlp, (tr_loader, va_loader, te_loader), train_cfg, task)\n",
        "        models.append(mlp)\n",
        "        val_scores.append(score)\n",
        "        torch.save(mlp.state_dict(), os.path.join(out_dir, f\"mlp_{i}.pt\"))\n",
        "\n",
        "    # dump embeddings\n",
        "    emb_train = dump_embeddings(models, DataLoader(TabularDataset(trX, trY), batch_size=train_cfg.batch_size, shuffle=False),\n",
        "                                task, train_cfg.device)\n",
        "    emb_val   = dump_embeddings(models, DataLoader(TabularDataset(vaX, vaY), batch_size=train_cfg.batch_size, shuffle=False),\n",
        "                                task, train_cfg.device)\n",
        "    emb_test  = dump_embeddings(models, te_loader, task, train_cfg.device)\n",
        "\n",
        "    np.save(os.path.join(out_dir, \"emb_train.npy\"), emb_train)\n",
        "    np.save(os.path.join(out_dir, \"emb_val.npy\"),   emb_val)\n",
        "    np.save(os.path.join(out_dir, \"emb_test.npy\"),  emb_test)\n",
        "\n",
        "    # save metadata for the next stage (Transformer)\n",
        "    with open(os.path.join(out_dir, \"meta.json\"), \"w\") as f:\n",
        "        json.dump({\n",
        "            \"task\": task,\n",
        "            \"in_dim\": in_dim,\n",
        "            \"embedding_dim\": int(emb_train.shape[-1]),\n",
        "            \"n_mlps\": train_cfg.n_mlps,\n",
        "            \"feature_names\": prep.feature_names_,\n",
        "            \"val_scores\": val_scores\n",
        "        }, f, indent=2)\n",
        "\n",
        "    return {\n",
        "        \"prep\": prep,\n",
        "        \"models\": models,\n",
        "        \"paths\": {\n",
        "            \"emb_train\": os.path.join(out_dir, \"emb_train.npy\"),\n",
        "            \"emb_val\":   os.path.join(out_dir, \"emb_val.npy\"),\n",
        "            \"emb_test\":  os.path.join(out_dir, \"emb_test.npy\"),\n",
        "            \"meta\":      os.path.join(out_dir, \"meta.json\")\n",
        "        }\n",
        "    }\n",
        "\n",
        "# ----------------------------- USAGE EXAMPLE -----------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Example: load any CSV and specify target column\n",
        "    df = pd.read_csv(\"adult.csv\")\n",
        "    result = train_mlps_and_save_embeddings(df, target_col=\"income\")\n",
        "    pass\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyXOcgbqrDyz",
        "outputId": "a67b5436-d6c2-475a-ba86-35ff063e15d6"
      },
      "id": "JyXOcgbqrDyz",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello1\n",
            "hello2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "w7CkAcE-gHH9"
      },
      "id": "w7CkAcE-gHH9",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_csv('covtype.csv').head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "Gnj4fSesfk9H",
        "outputId": "4da0510b-9b7f-45f1-e950-4d92447a76a6"
      },
      "id": "Gnj4fSesfk9H",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Elevation  Aspect  Slope  Horizontal_Distance_To_Hydrology  \\\n",
              "0       2596      51      3                               258   \n",
              "1       2590      56      2                               212   \n",
              "2       2804     139      9                               268   \n",
              "3       2785     155     18                               242   \n",
              "4       2595      45      2                               153   \n",
              "\n",
              "   Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  \\\n",
              "0                               0                            510.0   \n",
              "1                              -6                            390.0   \n",
              "2                              65                           3180.0   \n",
              "3                             118                           3090.0   \n",
              "4                              -1                            391.0   \n",
              "\n",
              "   Hillshade_9am  Hillshade_Noon  Hillshade_3pm  \\\n",
              "0          221.0           232.0          148.0   \n",
              "1          220.0           235.0          151.0   \n",
              "2          234.0           238.0          135.0   \n",
              "3          238.0           238.0          122.0   \n",
              "4          220.0           234.0          150.0   \n",
              "\n",
              "   Horizontal_Distance_To_Fire_Points  ...  Soil_Type32  Soil_Type33  \\\n",
              "0                              6279.0  ...          0.0          0.0   \n",
              "1                              6225.0  ...          0.0          0.0   \n",
              "2                              6121.0  ...          0.0          0.0   \n",
              "3                              6211.0  ...          0.0          0.0   \n",
              "4                              6172.0  ...          0.0          0.0   \n",
              "\n",
              "   Soil_Type34  Soil_Type35  Soil_Type36  Soil_Type37  Soil_Type38  \\\n",
              "0          0.0          0.0          0.0          0.0          0.0   \n",
              "1          0.0          0.0          0.0          0.0          0.0   \n",
              "2          0.0          0.0          0.0          0.0          0.0   \n",
              "3          0.0          0.0          0.0          0.0          0.0   \n",
              "4          0.0          0.0          0.0          0.0          0.0   \n",
              "\n",
              "   Soil_Type39  Soil_Type40  Cover_Type  \n",
              "0          0.0          0.0         5.0  \n",
              "1          0.0          0.0         5.0  \n",
              "2          0.0          0.0         2.0  \n",
              "3          0.0          0.0         2.0  \n",
              "4          0.0          0.0         5.0  \n",
              "\n",
              "[5 rows x 55 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-306e5950-2305-4b54-a4fb-683dd5f2e1ac\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Elevation</th>\n",
              "      <th>Aspect</th>\n",
              "      <th>Slope</th>\n",
              "      <th>Horizontal_Distance_To_Hydrology</th>\n",
              "      <th>Vertical_Distance_To_Hydrology</th>\n",
              "      <th>Horizontal_Distance_To_Roadways</th>\n",
              "      <th>Hillshade_9am</th>\n",
              "      <th>Hillshade_Noon</th>\n",
              "      <th>Hillshade_3pm</th>\n",
              "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
              "      <th>...</th>\n",
              "      <th>Soil_Type32</th>\n",
              "      <th>Soil_Type33</th>\n",
              "      <th>Soil_Type34</th>\n",
              "      <th>Soil_Type35</th>\n",
              "      <th>Soil_Type36</th>\n",
              "      <th>Soil_Type37</th>\n",
              "      <th>Soil_Type38</th>\n",
              "      <th>Soil_Type39</th>\n",
              "      <th>Soil_Type40</th>\n",
              "      <th>Cover_Type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2596</td>\n",
              "      <td>51</td>\n",
              "      <td>3</td>\n",
              "      <td>258</td>\n",
              "      <td>0</td>\n",
              "      <td>510.0</td>\n",
              "      <td>221.0</td>\n",
              "      <td>232.0</td>\n",
              "      <td>148.0</td>\n",
              "      <td>6279.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2590</td>\n",
              "      <td>56</td>\n",
              "      <td>2</td>\n",
              "      <td>212</td>\n",
              "      <td>-6</td>\n",
              "      <td>390.0</td>\n",
              "      <td>220.0</td>\n",
              "      <td>235.0</td>\n",
              "      <td>151.0</td>\n",
              "      <td>6225.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2804</td>\n",
              "      <td>139</td>\n",
              "      <td>9</td>\n",
              "      <td>268</td>\n",
              "      <td>65</td>\n",
              "      <td>3180.0</td>\n",
              "      <td>234.0</td>\n",
              "      <td>238.0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>6121.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2785</td>\n",
              "      <td>155</td>\n",
              "      <td>18</td>\n",
              "      <td>242</td>\n",
              "      <td>118</td>\n",
              "      <td>3090.0</td>\n",
              "      <td>238.0</td>\n",
              "      <td>238.0</td>\n",
              "      <td>122.0</td>\n",
              "      <td>6211.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2595</td>\n",
              "      <td>45</td>\n",
              "      <td>2</td>\n",
              "      <td>153</td>\n",
              "      <td>-1</td>\n",
              "      <td>391.0</td>\n",
              "      <td>220.0</td>\n",
              "      <td>234.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>6172.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 55 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-306e5950-2305-4b54-a4fb-683dd5f2e1ac')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-306e5950-2305-4b54-a4fb-683dd5f2e1ac button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-306e5950-2305-4b54-a4fb-683dd5f2e1ac');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-d90921bd-ae27-48bc-af3d-d216c94977af\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d90921bd-ae27-48bc-af3d-d216c94977af')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-d90921bd-ae27-48bc-af3d-d216c94977af button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lAz8NrLUgFnz"
      },
      "id": "lAz8NrLUgFnz",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}