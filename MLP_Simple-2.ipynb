{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e575dc0",
      "metadata": {
        "id": "9e575dc0"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
        "# from sklearn.impute import SimpleImputer\n",
        "# from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# class DataPreprocessor:\n",
        "#     def __init__(self,\n",
        "#                  use_one_hot_encoding=False,\n",
        "#                  use_mean_imputation=False,\n",
        "#                  use_mode_imputation=False,\n",
        "#                  use_knn_imputation=False,\n",
        "#                  use_standardization=False,\n",
        "#                  use_min_max_scaling=False,\n",
        "#                  use_noise_injection=False,\n",
        "#                  noise_factor=0.01):\n",
        "\n",
        "#         self.use_one_hot_encoding = use_one_hot_encoding\n",
        "#         self.use_mean_imputation = use_mean_imputation\n",
        "#         self.use_mode_imputation = use_mode_imputation\n",
        "#         self.use_knn_imputation = use_knn_imputation\n",
        "#         self.use_standardization = use_standardization\n",
        "#         self.use_min_max_scaling = use_min_max_scaling\n",
        "#         self.use_noise_injection = use_noise_injection\n",
        "#         self.noise_factor = noise_factor\n",
        "\n",
        "#         # Initialize transformers\n",
        "#         self.ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)  # Fix here\n",
        "#         self.imputer_mean = SimpleImputer(strategy='mean')\n",
        "#         self.imputer_mode = SimpleImputer(strategy='most_frequent')\n",
        "#         self.imputer_knn = KNeighborsClassifier(n_neighbors=5)\n",
        "#         self.scaler_standard = StandardScaler()\n",
        "#         self.scaler_minmax = MinMaxScaler()\n",
        "\n",
        "#     def fit_transform(self, X):\n",
        "#         # Automatically detect categorical and numerical columns\n",
        "#         categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
        "#         numerical_cols = X.select_dtypes(exclude=['object', 'category']).columns\n",
        "\n",
        "#         # Handle categorical columns with One-Hot Encoding\n",
        "#         if self.use_one_hot_encoding and len(categorical_cols) > 0:\n",
        "#             X_cat = X[categorical_cols]\n",
        "#             X_encoded = self.ohe.fit_transform(X_cat)\n",
        "#             X_encoded_df = pd.DataFrame(X_encoded, columns=self.ohe.get_feature_names_out(categorical_cols))\n",
        "#             X = X.drop(columns=categorical_cols)\n",
        "#             X = pd.concat([X, X_encoded_df], axis=1)\n",
        "\n",
        "#         # Handle Missing Data Imputation\n",
        "#         if self.use_mean_imputation and len(numerical_cols) > 0:\n",
        "#             X[numerical_cols] = self.imputer_mean.fit_transform(X[numerical_cols])\n",
        "#         if self.use_mode_imputation and len(categorical_cols) > 0:\n",
        "#             X[categorical_cols] = self.imputer_mode.fit_transform(X[categorical_cols])\n",
        "#         if self.use_knn_imputation:\n",
        "#             X = self.knn_imputation(X)\n",
        "\n",
        "#         # Feature Scaling\n",
        "#         if self.use_standardization and len(numerical_cols) > 0:\n",
        "#             X[numerical_cols] = self.scaler_standard.fit_transform(X[numerical_cols])\n",
        "#         if self.use_min_max_scaling and len(numerical_cols) > 0:\n",
        "#             X[numerical_cols] = self.scaler_minmax.fit_transform(X[numerical_cols])\n",
        "\n",
        "#         # Noise Injection for Robustness\n",
        "#         if self.use_noise_injection:\n",
        "#             X = self.inject_noise(X)\n",
        "\n",
        "#         return X\n",
        "\n",
        "#     def transform(self, X):\n",
        "#         # Automatically detect categorical and numerical columns\n",
        "#         categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
        "#         numerical_cols = X.select_dtypes(exclude=['object', 'category']).columns\n",
        "\n",
        "#         # Handle categorical columns with One-Hot Encoding\n",
        "#         if self.use_one_hot_encoding and len(categorical_cols) > 0:\n",
        "#             X_cat = X[categorical_cols]\n",
        "#             X_encoded = self.ohe.transform(X_cat)\n",
        "#             X_encoded_df = pd.DataFrame(X_encoded, columns=self.ohe.get_feature_names_out(categorical_cols))\n",
        "#             X = X.drop(columns=categorical_cols)\n",
        "#             X = pd.concat([X, X_encoded_df], axis=1)\n",
        "\n",
        "#         # Handle Missing Data Imputation\n",
        "#         if self.use_mean_imputation and len(numerical_cols) > 0:\n",
        "#             X[numerical_cols] = self.imputer_mean.transform(X[numerical_cols])\n",
        "#         if self.use_mode_imputation and len(categorical_cols) > 0:\n",
        "#             X[categorical_cols] = self.imputer_mode.transform(X[categorical_cols])\n",
        "#         if self.use_knn_imputation:\n",
        "#             X = self.knn_imputation(X)\n",
        "\n",
        "#         # Feature Scaling\n",
        "#         if self.use_standardization and len(numerical_cols) > 0:\n",
        "#             X[numerical_cols] = self.scaler_standard.transform(X[numerical_cols])\n",
        "#         if self.use_min_max_scaling and len(numerical_cols) > 0:\n",
        "#             X[numerical_cols] = self.scaler_minmax.transform(X[numerical_cols])\n",
        "\n",
        "#         # Noise Injection for Robustness\n",
        "#         if self.use_noise_injection:\n",
        "#             X = self.inject_noise(X)\n",
        "\n",
        "#         return X\n",
        "\n",
        "#     def knn_imputation(self, X):\n",
        "#         # KNN Imputation for missing values\n",
        "#         X_filled = X.copy()\n",
        "#         for col in X.columns:\n",
        "#             if X[col].isnull().any():\n",
        "#                 knn = self.imputer_knn.fit(X.dropna())\n",
        "#                 X_filled[col] = knn.predict(X[col].dropna().values.reshape(-1, 1))\n",
        "#         return X_filled\n",
        "\n",
        "#     def inject_noise(self, X):\n",
        "#         # Inject random noise into the numerical features\n",
        "#         noisy_X = X.copy()\n",
        "#         numeric_cols = noisy_X.select_dtypes(include=[np.number]).columns\n",
        "#         noise = np.random.normal(0, self.noise_factor, size=noisy_X[numeric_cols].shape)\n",
        "#         noisy_X[numeric_cols] += noise\n",
        "#         return noisy_X\n",
        "\n",
        "\n",
        "# # Example usage\n",
        "# if __name__ == \"__main__\":\n",
        "#     # Generate some sample data\n",
        "#     data = pd.read_csv(r'covtype.csv')\n",
        "\n",
        "#     # Initialize preprocessor\n",
        "#     preprocessor = DataPreprocessor(\n",
        "#         use_one_hot_encoding=True,\n",
        "#         use_mean_imputation=True,\n",
        "#         use_standardization=True,\n",
        "#         use_noise_injection=False,\n",
        "#         noise_factor=0.01\n",
        "#     )\n",
        "\n",
        "#     # Preprocess the data\n",
        "#     processed_data = preprocessor.fit_transform(data)\n",
        "#     print(processed_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2688941a",
      "metadata": {
        "id": "2688941a"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score, f1_score, mean_squared_error\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "\n",
        "# # Download dataset (replace with the actual dataset path on your system)\n",
        "# # For example:\n",
        "# # data = pd.read_csv('path_to_forest_cover_type.csv')\n",
        "\n",
        "# # Assuming the data is already loaded as 'data'\n",
        "\n",
        "# class AutoMLP:\n",
        "#     def __init__(self, data, target_col, hidden_dims=[64, 32], batch_size=64, epochs=100, learning_rate=0.001, noise_factor=0.01):\n",
        "#         \"\"\"\n",
        "#         Initialize the AutoMLP class for automatic classification or regression task.\n",
        "\n",
        "#         :param data: pandas DataFrame, contains the input features and target column\n",
        "#         :param target_col: str, name of the target column\n",
        "#         :param hidden_dims: list of int, sizes of the hidden layers\n",
        "#         :param batch_size: int, batch size for training\n",
        "#         :param epochs: int, number of training epochs\n",
        "#         :param learning_rate: float, learning rate for the optimizer\n",
        "#         :param noise_factor: float, factor for noise injection\n",
        "#         \"\"\"\n",
        "#         self.data = data\n",
        "#         self.target_col = target_col\n",
        "#         self.hidden_dims = hidden_dims\n",
        "#         self.batch_size = batch_size\n",
        "#         self.epochs = epochs\n",
        "#         self.learning_rate = learning_rate\n",
        "#         self.noise_factor = noise_factor\n",
        "\n",
        "#         # Separate features and target\n",
        "#         self.X = self.data.drop(columns=[self.target_col])\n",
        "#         self.y = self.data[self.target_col]\n",
        "\n",
        "#         # Detect task type\n",
        "#         self.is_classification = self._detect_task_type()\n",
        "#         self.output_dim = len(np.unique(self.y)) if self.is_classification else 1\n",
        "\n",
        "#         # Train-test split\n",
        "#         self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size=0.2, random_state=42)\n",
        "\n",
        "#         # Create model\n",
        "#         self.model = self._build_model()\n",
        "\n",
        "#     def _detect_task_type(self):\n",
        "#         \"\"\"Detect if the task is classification or regression based on the target column.\"\"\"\n",
        "#         if len(np.unique(self.y)) <= 10 and np.all(self.y.apply(lambda x: isinstance(x, (int, np.integer)))):\n",
        "#             return True  # Classification (less than or equal to 10 unique values, integers)\n",
        "#         return False  # Regression\n",
        "\n",
        "#     def _build_model(self):\n",
        "#         \"\"\"Build the MLP model.\"\"\"\n",
        "#         input_dim = self.X.shape[1]\n",
        "\n",
        "#         layers = []\n",
        "#         layers.append(nn.Linear(input_dim, self.hidden_dims[0]))\n",
        "#         layers.append(nn.ReLU())\n",
        "\n",
        "#         # Hidden layers\n",
        "#         for i in range(1, len(self.hidden_dims)):\n",
        "#             layers.append(nn.Linear(self.hidden_dims[i-1], self.hidden_dims[i]))\n",
        "#             layers.append(nn.ReLU())\n",
        "\n",
        "#         # Output layer\n",
        "#         layers.append(nn.Linear(self.hidden_dims[-1], self.output_dim))\n",
        "\n",
        "#         # For classification, apply Softmax activation on the output layer\n",
        "#         if self.is_classification:\n",
        "#             self.final_activation = nn.Softmax(dim=1)\n",
        "#         else:\n",
        "#             self.final_activation = nn.Identity()\n",
        "\n",
        "#         model = nn.Sequential(*layers)\n",
        "#         return model\n",
        "\n",
        "#     def train_model(self):\n",
        "#         \"\"\"Train the model.\"\"\"\n",
        "#         # Convert to PyTorch tensors\n",
        "#         X_train_tensor = torch.tensor(self.X_train.values, dtype=torch.float32)\n",
        "#         y_train_tensor = torch.tensor(self.y_train.values, dtype=torch.long if self.is_classification else torch.float32).view(-1, 1)\n",
        "\n",
        "#         # Loss function and optimizer\n",
        "#         loss_function = nn.CrossEntropyLoss() if self.is_classification else nn.MSELoss()\n",
        "#         optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "\n",
        "#         # Training loop\n",
        "#         for epoch in range(self.epochs):\n",
        "#             self.model.train()\n",
        "#             optimizer.zero_grad()\n",
        "\n",
        "#             # Forward pass\n",
        "#             outputs = self.model(X_train_tensor)\n",
        "#             loss = loss_function(outputs, y_train_tensor.squeeze())  # Remove extra dimension\n",
        "\n",
        "#             # Backward pass and optimization\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "\n",
        "#             if (epoch + 1) % 10 == 0:\n",
        "#                 print(f\"Epoch [{epoch+1}/{self.epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "#     def evaluate(self):\n",
        "#         \"\"\"Evaluate the model and return metrics.\"\"\"\n",
        "#         # Convert to PyTorch tensors\n",
        "#         X_test_tensor = torch.tensor(self.X_test.values, dtype=torch.float32)\n",
        "#         y_test_tensor = torch.tensor(self.y_test.values, dtype=torch.long if self.is_classification else torch.float32).view(-1, 1)\n",
        "\n",
        "#         # Evaluate the model\n",
        "#         self.model.eval()\n",
        "#         with torch.no_grad():\n",
        "#             outputs = self.model(X_test_tensor)\n",
        "#             outputs = self.final_activation(outputs)  # Apply final activation (Softmax for classification)\n",
        "\n",
        "#             # For classification, use argmax to get predicted classes\n",
        "#             if self.is_classification:\n",
        "#                 predicted = torch.argmax(outputs, dim=1)\n",
        "#                 self._print_classification_metrics(predicted, y_test_tensor)\n",
        "#             else:\n",
        "#                 predicted = outputs\n",
        "#                 self._print_regression_metrics(predicted, y_test_tensor)\n",
        "\n",
        "#             return predicted.numpy(), y_test_tensor.numpy()\n",
        "\n",
        "#     def _print_classification_metrics(self, predicted, true):\n",
        "#         \"\"\"Print classification metrics (accuracy, precision, recall, F1 score).\"\"\"\n",
        "#         accuracy = accuracy_score(true, predicted)\n",
        "#         precision = precision_score(true, predicted, average='weighted', zero_division=0)\n",
        "#         recall = recall_score(true, predicted, average='weighted', zero_division=0)\n",
        "#         f1 = f1_score(true, predicted, average='weighted', zero_division=0)\n",
        "\n",
        "#         print(f\"Accuracy: {accuracy:.4f}\")\n",
        "#         print(f\"Precision: {precision:.4f}\")\n",
        "#         print(f\"Recall: {recall:.4f}\")\n",
        "#         print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "#     def _print_regression_metrics(self, predicted, true):\n",
        "#         \"\"\"Print regression metrics (Mean Squared Error).\"\"\"\n",
        "#         mse = mean_squared_error(true, predicted)\n",
        "#         print(f\"Mean Squared Error: {mse:.4f}\")\n",
        "\n",
        "\n",
        "# # Example usage:\n",
        "# if __name__ == \"__main__\":\n",
        "#     # Load the dataset (replace with the actual dataset path or Kaggle API call)\n",
        "#     # For demonstration, using a dataset like 'load_iris' here\n",
        "\n",
        "#     data = pd.read_csv(r'C:\\Users\\Shaikh\\Documents\\SRP\\covtype.csv')\n",
        "\n",
        "\n",
        "#     # Initialize preprocessor\n",
        "#     preprocessor = DataPreprocessor(\n",
        "#         use_one_hot_encoding=True,\n",
        "#         use_mean_imputation=True,\n",
        "#         use_standardization=True,\n",
        "#         use_noise_injection=False,\n",
        "#         noise_factor=0.01\n",
        "#     )\n",
        "\n",
        "#     # Preprocess the data\n",
        "#     X_processed = preprocessor.fit_transform(data.drop(columns=['target']))\n",
        "#     data['target'] = data['Cover_Type']  # This should be the target column\n",
        "\n",
        "#     # Initialize AutoMLP (this will automatically detect the task type)\n",
        "#     model = AutoMLP(data=data, target_col='target', hidden_dims=[64, 32], epochs=100)\n",
        "\n",
        "#     # Train the model\n",
        "#     model.train_model()\n",
        "\n",
        "#     # Evaluate the model (for classification: precision, recall, accuracy, F1 score)\n",
        "#     predictions, true_labels = model.evaluate()\n",
        "#     print(\"Predictions:\", predictions)\n",
        "#     print(\"True labels:\", true_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "698f0b50",
      "metadata": {
        "id": "698f0b50"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
        "# from sklearn.impute import SimpleImputer\n",
        "# from sklearn.neighbors import KNeighborsClassifier\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, mean_squared_error\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "\n",
        "# # DataPreprocessor Class\n",
        "# class DataPreprocessor:\n",
        "#     def __init__(self,\n",
        "#                  use_one_hot_encoding=False,\n",
        "#                  use_mean_imputation=False,\n",
        "#                  use_mode_imputation=False,\n",
        "#                  use_knn_imputation=False,\n",
        "#                  use_standardization=False,\n",
        "#                  use_min_max_scaling=False,\n",
        "#                  use_noise_injection=False,\n",
        "#                  noise_factor=0.01):\n",
        "\n",
        "#         self.use_one_hot_encoding = use_one_hot_encoding\n",
        "#         self.use_mean_imputation = use_mean_imputation\n",
        "#         self.use_mode_imputation = use_mode_imputation\n",
        "#         self.use_knn_imputation = use_knn_imputation\n",
        "#         self.use_standardization = use_standardization\n",
        "#         self.use_min_max_scaling = use_min_max_scaling\n",
        "#         self.use_noise_injection = use_noise_injection\n",
        "#         self.noise_factor = noise_factor\n",
        "\n",
        "#         self.ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "#         self.imputer_mean = SimpleImputer(strategy='mean')\n",
        "#         self.imputer_mode = SimpleImputer(strategy='most_frequent')\n",
        "#         self.imputer_knn = KNeighborsClassifier(n_neighbors=5)\n",
        "#         self.scaler_standard = StandardScaler()\n",
        "#         self.scaler_minmax = MinMaxScaler()\n",
        "\n",
        "#     def fit_transform(self, X):\n",
        "#         categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
        "#         numerical_cols = X.select_dtypes(exclude=['object', 'category']).columns\n",
        "\n",
        "#         # Imputation\n",
        "#         if self.use_mean_imputation and len(numerical_cols) > 0:\n",
        "#             X[numerical_cols] = self.imputer_mean.fit_transform(X[numerical_cols])\n",
        "#         if self.use_mode_imputation and len(categorical_cols) > 0:\n",
        "#             X[categorical_cols] = self.imputer_mode.fit_transform(X[categorical_cols])\n",
        "\n",
        "#         # One-hot encoding\n",
        "#         if self.use_one_hot_encoding and len(categorical_cols) > 0:\n",
        "#             X_cat = X[categorical_cols]\n",
        "#             X_encoded = self.ohe.fit_transform(X_cat)\n",
        "#             X_encoded_df = pd.DataFrame(X_encoded, columns=self.ohe.get_feature_names_out(categorical_cols), index=X.index)\n",
        "#             X = X.drop(columns=categorical_cols)\n",
        "#             X = pd.concat([X, X_encoded_df], axis=1)\n",
        "\n",
        "#         # Scaling\n",
        "#         if self.use_standardization and len(numerical_cols) > 0:\n",
        "#             X[numerical_cols] = self.scaler_standard.fit_transform(X[numerical_cols])\n",
        "#         elif self.use_min_max_scaling and len(numerical_cols) > 0:\n",
        "#             X[numerical_cols] = self.scaler_minmax.fit_transform(X[numerical_cols])\n",
        "\n",
        "#         # Noise injection\n",
        "#         if self.use_noise_injection:\n",
        "#             X = self.inject_noise(X)\n",
        "\n",
        "#         return X\n",
        "\n",
        "#     def transform(self, X):\n",
        "#         categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
        "#         numerical_cols = X.select_dtypes(exclude=['object', 'category']).columns\n",
        "\n",
        "#         if self.use_mean_imputation and len(numerical_cols) > 0:\n",
        "#             X[numerical_cols] = self.imputer_mean.transform(X[numerical_cols])\n",
        "#         if self.use_mode_imputation and len(categorical_cols) > 0:\n",
        "#             X[categorical_cols] = self.imputer_mode.transform(X[categorical_cols])\n",
        "\n",
        "#         if self.use_one_hot_encoding and len(categorical_cols) > 0:\n",
        "#             X_cat = X[categorical_cols]\n",
        "#             X_encoded = self.ohe.transform(X_cat)\n",
        "#             X_encoded_df = pd.DataFrame(X_encoded, columns=self.ohe.get_feature_names_out(categorical_cols), index=X.index)\n",
        "#             X = X.drop(columns=categorical_cols)\n",
        "#             X = pd.concat([X, X_encoded_df], axis=1)\n",
        "\n",
        "#         if self.use_standardization and len(numerical_cols) > 0:\n",
        "#             X[numerical_cols] = self.scaler_standard.transform(X[numerical_cols])\n",
        "#         elif self.use_min_max_scaling and len(numerical_cols) > 0:\n",
        "#             X[numerical_cols] = self.scaler_minmax.transform(X[numerical_cols])\n",
        "\n",
        "#         if self.use_noise_injection:\n",
        "#             X = self.inject_noise(X)\n",
        "\n",
        "#         return X\n",
        "\n",
        "#     def inject_noise(self, X):\n",
        "#         noisy_X = X.copy()\n",
        "#         numeric_cols = noisy_X.select_dtypes(include=[np.number]).columns\n",
        "#         noise = np.random.normal(0, self.noise_factor, size=noisy_X[numeric_cols].shape)\n",
        "#         noisy_X[numeric_cols] += noise\n",
        "#         return noisy_X\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6f79a8e",
      "metadata": {
        "id": "e6f79a8e"
      },
      "outputs": [],
      "source": [
        "# # AutoMLP Class\n",
        "# class AutoMLP:\n",
        "#     def __init__(self, X, y, hidden_dims=[64, 32], batch_size=64, epochs=140, learning_rate=0.001):\n",
        "#         \"\"\"\n",
        "#         :param X: preprocessed pandas DataFrame of features (all numeric)\n",
        "#         :param y: pandas Series or numpy array of target values (classification or regression)\n",
        "#         \"\"\"\n",
        "#         self.X = X\n",
        "#         self.y = y\n",
        "\n",
        "#         self.hidden_dims = hidden_dims\n",
        "#         self.batch_size = batch_size\n",
        "#         self.epochs = epochs\n",
        "#         self.learning_rate = learning_rate\n",
        "\n",
        "#         self.is_classification = self._detect_task_type()\n",
        "#         self.output_dim = len(np.unique(self.y)) if self.is_classification else 1\n",
        "\n",
        "#         self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
        "#             self.X, self.y, test_size=0.2, random_state=42)\n",
        "\n",
        "#         self.model = self._build_model()\n",
        "\n",
        "#     def _detect_task_type(self):\n",
        "#         if len(np.unique(self.y)) <= 10 and np.all(np.mod(self.y, 1) == 0):\n",
        "#             return True\n",
        "#         return False\n",
        "\n",
        "#     def _build_model(self):\n",
        "#         input_dim = self.X.shape[1]\n",
        "#         layers = []\n",
        "#         layers.append(nn.Linear(input_dim, self.hidden_dims[0]))\n",
        "#         layers.append(nn.ReLU())\n",
        "#         for i in range(1, len(self.hidden_dims)):\n",
        "#             layers.append(nn.Linear(self.hidden_dims[i-1], self.hidden_dims[i]))\n",
        "#             layers.append(nn.ReLU())\n",
        "#         layers.append(nn.Linear(self.hidden_dims[-1], self.output_dim))\n",
        "#         if self.is_classification:\n",
        "#             self.final_activation = nn.Softmax(dim=1)\n",
        "#         else:\n",
        "#             self.final_activation = nn.Identity()\n",
        "#         return nn.Sequential(*layers)\n",
        "\n",
        "#     def train_model(self):\n",
        "#         X_train_tensor = torch.tensor(self.X_train.values, dtype=torch.float32)\n",
        "#         y_train_tensor = torch.tensor(self.y_train.values, dtype=torch.long if self.is_classification else torch.float32)\n",
        "\n",
        "#         loss_fn = nn.CrossEntropyLoss() if self.is_classification else nn.MSELoss()\n",
        "#         optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "\n",
        "#         for epoch in range(self.epochs):\n",
        "#             self.model.train()\n",
        "#             optimizer.zero_grad()\n",
        "#             outputs = self.model(X_train_tensor)\n",
        "#             if self.is_classification:\n",
        "#                 loss = loss_fn(outputs, y_train_tensor)\n",
        "#             else:\n",
        "#                 loss = loss_fn(outputs.squeeze(), y_train_tensor)\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "#             if (epoch + 1) % 10 == 0:\n",
        "#                 print(f\"Epoch [{epoch+1}/{self.epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "#     def evaluate(self):\n",
        "#         X_test_tensor = torch.tensor(self.X_test.values, dtype=torch.float32)\n",
        "#         y_test_tensor = torch.tensor(self.y_test.values, dtype=torch.long if self.is_classification else torch.float32)\n",
        "\n",
        "#         self.model.eval()\n",
        "#         with torch.no_grad():\n",
        "#             outputs = self.model(X_test_tensor)\n",
        "#             outputs = self.final_activation(outputs)\n",
        "#             if self.is_classification:\n",
        "#                 preds = torch.argmax(outputs, dim=1).numpy()\n",
        "#                 self._print_classification_metrics(preds, y_test_tensor.numpy())\n",
        "#             else:\n",
        "#                 preds = outputs.squeeze().numpy()\n",
        "#                 self._print_regression_metrics(preds, y_test_tensor.numpy())\n",
        "#             return preds, y_test_tensor.numpy()\n",
        "\n",
        "#     def _print_classification_metrics(self, preds, true):\n",
        "#         from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "#         print(\"Classification Metrics:\")\n",
        "#         print(\"Accuracy:\", accuracy_score(true, preds))\n",
        "#         print(\"Precision:\", precision_score(true, preds, average='weighted', zero_division=0))\n",
        "#         print(\"Recall:\", recall_score(true, preds, average='weighted', zero_division=0))\n",
        "#         print(\"F1 Score:\", f1_score(true, preds, average='weighted', zero_division=0))\n",
        "\n",
        "#     def _print_regression_metrics(self, preds, true):\n",
        "#         from sklearn.metrics import mean_squared_error\n",
        "#         mse = mean_squared_error(true, preds)\n",
        "#         print(\"Regression Metrics:\")\n",
        "#         print(\"Mean Squared Error:\", mse)\n",
        "\n",
        "\n",
        "# # Usage example\n",
        "# if __name__ == \"__main__\":\n",
        "#     data = pd.read_csv(r'covtype.csv')\n",
        "\n",
        "#     target_col = 'Cover_Type'\n",
        "\n",
        "#     # Separate features and target\n",
        "#     X = data.drop(columns=[target_col])\n",
        "#     y = data[target_col]\n",
        "\n",
        "#     # Re-index target labels for PyTorch\n",
        "#     y = y - y.min()\n",
        "\n",
        "#     # Preprocess features only\n",
        "#     preprocessor = DataPreprocessor(\n",
        "#         use_one_hot_encoding=True,\n",
        "#         use_mean_imputation=True,\n",
        "#         use_standardization=True,\n",
        "#         use_noise_injection=False,\n",
        "#         noise_factor=0.01\n",
        "#     )\n",
        "#     X_processed = preprocessor.fit_transform(X)\n",
        "\n",
        "#     # Train and evaluate\n",
        "#     model = AutoMLP(X_processed, y, hidden_dims=[64, 32], epochs=100)\n",
        "#     model.train_model()\n",
        "#     preds, true = model.evaluate()\n",
        "\n",
        "#     print(\"Predictions:\", preds)\n",
        "#     print(\"True labels:\", true)\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# step1_multi_mlp_embeddings.py\n",
        "import os, json, math, numpy as np, pandas as pd\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple, List, Optional, Dict\n",
        "\n",
        "# ── sklearn: robust, generic preprocessing ──────────────────────────────────────\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
        "\n",
        "# ── torch: shallow MLPs + training ─────────────────────────────────────────────\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# ----------------------------- CONFIG ------------------------------------------\n",
        "@dataclass\n",
        "class PrepConfig:\n",
        "    impute_num: str = \"mean\"          # \"mean\" | \"median\" | \"knn\"\n",
        "    impute_cat: str = \"most_frequent\" # \"most_frequent\" | \"constant\"\n",
        "    scale: Optional[str] = \"standard\" # None | \"standard\" | \"minmax\"\n",
        "    noise_std: float = 0.0            # e.g., 0.01 for light noise on numeric features\n",
        "    knn_k: int = 5\n",
        "\n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "    n_mlps: int = 5\n",
        "    hidden_dim: int = 128\n",
        "    n_hidden_layers: int = 3          # shallow by default\n",
        "    dropout: float = 0.1\n",
        "    batch_size: int = 512\n",
        "    epochs: int = 35\n",
        "    lr: float = 1e-3\n",
        "    weight_decay: float = 1e-5\n",
        "    val_size: float = 0.2\n",
        "    test_size: float = 0.2\n",
        "    seed: int = 42\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    # early stopping\n",
        "    patience: int = 5\n",
        "\n",
        "# ----------------------- PREPROCESSOR (generic) --------------------------------\n",
        "class TabularPreprocessor:\n",
        "    def __init__(self, cfg: PrepConfig):\n",
        "        self.cfg = cfg\n",
        "        self.column_transformer: Optional[ColumnTransformer] = None\n",
        "        self.feature_names_: Optional[List[str]] = None\n",
        "        self.num_cols_: Optional[List[str]] = None\n",
        "        self.cat_cols_: Optional[List[str]] = None\n",
        "\n",
        "    def _num_imputer(self):\n",
        "        if self.cfg.impute_num == \"mean\":\n",
        "            return SimpleImputer(strategy=\"mean\")\n",
        "        if self.cfg.impute_num == \"median\":\n",
        "            return SimpleImputer(strategy=\"median\")\n",
        "        if self.cfg.impute_num == \"knn\":\n",
        "            return KNNImputer(n_neighbors=self.cfg.knn_k)\n",
        "        raise ValueError(\"impute_num must be mean|median|knn\")\n",
        "\n",
        "    def _scaler(self):\n",
        "        if self.cfg.scale is None:\n",
        "            return \"passthrough\"\n",
        "        if self.cfg.scale == \"standard\":\n",
        "            return StandardScaler()\n",
        "        if self.cfg.scale == \"minmax\":\n",
        "            return MinMaxScaler()\n",
        "        raise ValueError(\"scale must be None|standard|minmax\")\n",
        "\n",
        "    def fit(self, X: pd.DataFrame):\n",
        "        # Treat bools as categorical (common in tabular datasets)\n",
        "        self.cat_cols_ = X.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
        "        self.num_cols_ = X.columns.difference(self.cat_cols_).tolist()\n",
        "\n",
        "        num_pipe = Pipeline(steps=[\n",
        "            (\"imputer\", self._num_imputer()),\n",
        "            (\"scaler\", self._scaler()),\n",
        "        ])\n",
        "\n",
        "        cat_pipe = Pipeline(steps=[\n",
        "            (\"imputer\", SimpleImputer(strategy=self.cfg.impute_cat, fill_value=\"missing\")),\n",
        "            (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
        "        ])\n",
        "\n",
        "        self.column_transformer = ColumnTransformer(\n",
        "            transformers=[\n",
        "                (\"num\", num_pipe, self.num_cols_),\n",
        "                (\"cat\", cat_pipe, self.cat_cols_)\n",
        "            ],\n",
        "            remainder=\"drop\",\n",
        "        )\n",
        "        self.column_transformer.fit(X)\n",
        "\n",
        "        # Build feature names after fit\n",
        "        names = []\n",
        "        if self.num_cols_:\n",
        "            names += self.num_cols_\n",
        "        if self.cat_cols_:\n",
        "            ohe = self.column_transformer.named_transformers_[\"cat\"][\"ohe\"]\n",
        "            names += ohe.get_feature_names_out(self.cat_cols_).tolist()\n",
        "        self.feature_names_ = names\n",
        "        return self\n",
        "\n",
        "    def transform(self, X: pd.DataFrame) -> np.ndarray:\n",
        "        Xt = self.column_transformer.transform(X)\n",
        "        Xt = np.asarray(Xt, dtype=np.float32)\n",
        "        # optional light Gaussian noise on numeric part (first len(num_cols_) columns after pipeline)\n",
        "        if self.cfg.noise_std and self.cfg.noise_std > 0 and len(self.num_cols_) > 0:\n",
        "            n_num = len(self.num_cols_)\n",
        "            noise = np.random.normal(0, self.cfg.noise_std, size=(Xt.shape[0], n_num)).astype(np.float32)\n",
        "            Xt[:, :n_num] += noise\n",
        "        return Xt\n",
        "\n",
        "# ----------------------------- TORCH DATASET -----------------------------------\n",
        "class TabularDataset(Dataset):\n",
        "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
        "        self.X = torch.from_numpy(X.astype(np.float32))\n",
        "        self.y = torch.from_numpy(y)\n",
        "\n",
        "    def __len__(self): return self.X.shape[0]\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "\n",
        "# ----------------------------- MODEL -------------------------------------------\n",
        "class ShallowMLP(nn.Module):\n",
        "    def __init__(self, in_dim: int, hidden: int, n_hidden_layers: int, dropout: float, out_dim: int, task: str):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        dim = in_dim\n",
        "        for _ in range(n_hidden_layers):\n",
        "            layers += [nn.Linear(dim, hidden), nn.ReLU(), nn.Dropout(dropout)]\n",
        "            dim = hidden\n",
        "        self.backbone = nn.Sequential(*layers) if layers else nn.Identity()\n",
        "        self.head = nn.Linear(dim, out_dim)\n",
        "        self.task = task  # \"binary\" | \"multiclass\" | \"regression\"\n",
        "\n",
        "    def forward(self, x, return_embedding: bool = False):\n",
        "        emb = self.backbone(x)\n",
        "        logits = self.head(emb)\n",
        "        if return_embedding:\n",
        "            return logits, emb\n",
        "        return logits\n",
        "\n",
        "# ----------------------------- UTIL --------------------------------------------\n",
        "def infer_task_and_outdim(y: np.ndarray) -> Tuple[str, int, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Returns (task, out_dim, y_torch_ready)\n",
        "    \"\"\"\n",
        "    if y.dtype.kind in {\"f\"} and len(np.unique(y)) > 10:\n",
        "        return \"regression\", 1, y.astype(np.float32).reshape(-1, 1)\n",
        "    # classification\n",
        "    classes = np.unique(y)\n",
        "    if len(classes) == 2:\n",
        "        # map to {0,1}\n",
        "        mapping = {classes[0]: 0, classes[1]: 1}\n",
        "        y_ = np.vectorize(mapping.get)(y).astype(np.int64)\n",
        "        return \"binary\", 1, y_.reshape(-1)\n",
        "    else:\n",
        "        # 0..K-1\n",
        "        mapping = {c: i for i, c in enumerate(classes)}\n",
        "        y_ = np.vectorize(mapping.get)(y).astype(np.int64)\n",
        "        return \"multiclass\", len(classes), y_.reshape(-1)\n",
        "\n",
        "def make_loaders(Xtr, ytr, Xva, yva, Xte, yte, bs):\n",
        "    ds_tr, ds_va, ds_te = TabularDataset(Xtr, ytr), TabularDataset(Xva, yva), TabularDataset(Xte, yte)\n",
        "    return (DataLoader(ds_tr, batch_size=bs, shuffle=True),\n",
        "            DataLoader(ds_va, batch_size=bs, shuffle=False),\n",
        "            DataLoader(ds_te, batch_size=bs, shuffle=False))\n",
        "\n",
        "def make_loss(task: str):\n",
        "    if task == \"binary\":     return nn.BCEWithLogitsLoss()\n",
        "    if task == \"multiclass\": return nn.CrossEntropyLoss()\n",
        "    return nn.MSELoss()  # regression\n",
        "\n",
        "def metric_from_logits(task: str, logits: torch.Tensor, y: torch.Tensor) -> float:\n",
        "    if task == \"binary\":\n",
        "        preds = (torch.sigmoid(logits).view(-1) > 0.5).long()\n",
        "        return (preds == y.long()).float().mean().item()\n",
        "    if task == \"multiclass\":\n",
        "        preds = logits.argmax(dim=1)\n",
        "        return (preds == y.long()).float().mean().item()\n",
        "    # regression -> negative RMSE so higher is better for early stopping\n",
        "    mse = nn.functional.mse_loss(logits.view_as(y).float(), y.float()).item()\n",
        "    return -math.sqrt(mse)\n",
        "\n",
        "# ----------------------------- TRAIN ONE MLP -----------------------------------\n",
        "def train_one_mlp(model, loaders, cfg: TrainConfig, task: str):\n",
        "    tr_loader, va_loader, _ = loaders\n",
        "    device = cfg.device\n",
        "    model.to(device)\n",
        "    opt = optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "    loss_fn = make_loss(task)\n",
        "    best_score, best_state, patience = -1e9, None, cfg.patience\n",
        "\n",
        "    for epoch in range(cfg.epochs):\n",
        "        model.train()\n",
        "        for xb, yb in tr_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            opt.zero_grad()\n",
        "            logits = model(xb)\n",
        "            if task == \"binary\":\n",
        "                loss = loss_fn(logits.view(-1), yb.float())\n",
        "            elif task == \"multiclass\":\n",
        "                loss = loss_fn(logits, yb.long())\n",
        "            else:\n",
        "                loss = loss_fn(logits, yb.float())\n",
        "            loss.backward(); opt.step()\n",
        "\n",
        "        # validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            scores = []\n",
        "            for xb, yb in va_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                logits = model(xb)\n",
        "                scores.append(metric_from_logits(task, logits, yb))\n",
        "            score = float(np.mean(scores)) if scores else -1e9\n",
        "\n",
        "        if score > best_score + 1e-6:\n",
        "            best_score, best_state, patience = score, {k: v.cpu().clone() for k, v in model.state_dict().items()}, cfg.patience\n",
        "        else:\n",
        "            patience -= 1\n",
        "            if patience <= 0:\n",
        "                break\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "    return model, best_score\n",
        "\n",
        "# ----------------------------- EMBEDDING DUMP ----------------------------------\n",
        "@torch.no_grad()\n",
        "def dump_embeddings(models: List[ShallowMLP], loader: DataLoader, task: str, device: str) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Returns array (N_examples, n_mlps, emb_dim)\n",
        "    \"\"\"\n",
        "    # probe emb dim\n",
        "    for m in models: m.eval().to(device)\n",
        "    # run one batch to find embedding dimension\n",
        "    xb0, _ = next(iter(loader))\n",
        "    xb0 = xb0.to(device)\n",
        "    _, emb0 = models[0](xb0, return_embedding=True)\n",
        "    emb_dim = emb0.shape[1]\n",
        "    # collect\n",
        "    all_embs = [ [] for _ in models ]\n",
        "    for xb, _ in loader:\n",
        "        xb = xb.to(device)\n",
        "        for i, m in enumerate(models):\n",
        "            _, emb = m(xb, return_embedding=True)\n",
        "            all_embs[i].append(emb.cpu().numpy())\n",
        "    stacked = [np.concatenate(chunks, axis=0) for chunks in all_embs]  # list of (N, emb_dim)\n",
        "    return np.stack(stacked, axis=1)  # (N, n_mlps, emb_dim)\n",
        "\n",
        "# ----------------------------- MAIN ENTRY --------------------------------------\n",
        "def train_mlps_and_save_embeddings(\n",
        "    df: pd.DataFrame,\n",
        "    target_col: str,\n",
        "    out_dir: str = \"./artifacts_step1\",\n",
        "    prep_cfg: PrepConfig = PrepConfig(),\n",
        "    train_cfg: TrainConfig = TrainConfig(),\n",
        "):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    rng = np.random.RandomState(train_cfg.seed)\n",
        "\n",
        "    y_raw = df[target_col].values\n",
        "    X_raw = df.drop(columns=[target_col])\n",
        "    print('hello1')\n",
        "    # split (stratify if classification)\n",
        "    task0, _, _ = infer_task_and_outdim(y_raw)\n",
        "    if task0 in (\"binary\", \"multiclass\"):\n",
        "        strat = y_raw\n",
        "        splitter = StratifiedShuffleSplit(n_splits=1, test_size=train_cfg.test_size, random_state=train_cfg.seed)\n",
        "        train_idx, test_idx = next(splitter.split(X_raw, strat))\n",
        "    else:\n",
        "        idx = np.arange(len(df)); rng.shuffle(idx)\n",
        "        split = int(len(idx) * (1 - train_cfg.test_size))\n",
        "        train_idx, test_idx = idx[:split], idx[split:]\n",
        "\n",
        "    print('hello2')\n",
        "    X_train, X_test = X_raw.iloc[train_idx].copy(), X_raw.iloc[test_idx].copy()\n",
        "    y_train_raw, y_test_raw = y_raw[train_idx], y_raw[test_idx]\n",
        "\n",
        "    # build preprocessor on train, transform all\n",
        "    prep = TabularPreprocessor(prep_cfg).fit(X_train)\n",
        "    X_train_t = prep.transform(X_train)\n",
        "    X_test_t  = prep.transform(X_test)\n",
        "\n",
        "    # secondary split: train/val\n",
        "    if task0 in (\"binary\", \"multiclass\"):\n",
        "        trX, vaX, trY_raw, vaY_raw = train_test_split(\n",
        "            X_train_t, y_train_raw, test_size=train_cfg.val_size, random_state=train_cfg.seed, stratify=y_train_raw\n",
        "        )\n",
        "    else:\n",
        "        trX, vaX, trY_raw, vaY_raw = train_test_split(\n",
        "            X_train_t, y_train_raw, test_size=train_cfg.val_size, random_state=train_cfg.seed\n",
        "        )\n",
        "\n",
        "    # infer final task using training labels only\n",
        "    task, out_dim, trY = infer_task_and_outdim(trY_raw)\n",
        "    _, _, vaY = infer_task_and_outdim(vaY_raw)\n",
        "    _, _, teY = infer_task_and_outdim(y_test_raw)\n",
        "\n",
        "    tr_loader, va_loader, te_loader = make_loaders(trX, trY, vaX, vaY, X_test_t, teY, train_cfg.batch_size)\n",
        "\n",
        "    in_dim = trX.shape[1]\n",
        "    models: List[ShallowMLP] = []\n",
        "    val_scores: List[float] = []\n",
        "\n",
        "    for i in range(train_cfg.n_mlps):\n",
        "        torch.manual_seed(train_cfg.seed + i)\n",
        "        mlp = ShallowMLP(\n",
        "            in_dim=in_dim,\n",
        "            hidden=train_cfg.hidden_dim,\n",
        "            n_hidden_layers=train_cfg.n_hidden_layers,\n",
        "            dropout=train_cfg.dropout,\n",
        "            out_dim=out_dim,\n",
        "            task=task\n",
        "        )\n",
        "        mlp, score = train_one_mlp(mlp, (tr_loader, va_loader, te_loader), train_cfg, task)\n",
        "        models.append(mlp)\n",
        "        val_scores.append(score)\n",
        "        torch.save(mlp.state_dict(), os.path.join(out_dir, f\"mlp_{i}.pt\"))\n",
        "\n",
        "    # dump embeddings\n",
        "    emb_train = dump_embeddings(models, DataLoader(TabularDataset(trX, trY), batch_size=train_cfg.batch_size, shuffle=False),\n",
        "                                task, train_cfg.device)\n",
        "    emb_val   = dump_embeddings(models, DataLoader(TabularDataset(vaX, vaY), batch_size=train_cfg.batch_size, shuffle=False),\n",
        "                                task, train_cfg.device)\n",
        "    emb_test  = dump_embeddings(models, te_loader, task, train_cfg.device)\n",
        "\n",
        "    np.save(os.path.join(out_dir, \"emb_train.npy\"), emb_train)\n",
        "    np.save(os.path.join(out_dir, \"emb_val.npy\"),   emb_val)\n",
        "    np.save(os.path.join(out_dir, \"emb_test.npy\"),  emb_test)\n",
        "\n",
        "    # save metadata for the next stage (Transformer)\n",
        "    with open(os.path.join(out_dir, \"meta.json\"), \"w\") as f:\n",
        "        json.dump({\n",
        "            \"task\": task,\n",
        "            \"in_dim\": in_dim,\n",
        "            \"embedding_dim\": int(emb_train.shape[-1]),\n",
        "            \"n_mlps\": train_cfg.n_mlps,\n",
        "            \"feature_names\": prep.feature_names_,\n",
        "            \"val_scores\": val_scores\n",
        "        }, f, indent=2)\n",
        "\n",
        "    return {\n",
        "        \"prep\": prep,\n",
        "        \"models\": models,\n",
        "        \"paths\": {\n",
        "            \"emb_train\": os.path.join(out_dir, \"emb_train.npy\"),\n",
        "            \"emb_val\":   os.path.join(out_dir, \"emb_val.npy\"),\n",
        "            \"emb_test\":  os.path.join(out_dir, \"emb_test.npy\"),\n",
        "            \"meta\":      os.path.join(out_dir, \"meta.json\")\n",
        "        }\n",
        "    }\n",
        "\n",
        "# ----------------------------- USAGE EXAMPLE -----------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Example: load any CSV and specify target column\n",
        "    df = pd.read_csv(\"adult.csv\")\n",
        "    result = train_mlps_and_save_embeddings(df, target_col=\"income\")\n",
        "    pass\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyXOcgbqrDyz",
        "outputId": "a67b5436-d6c2-475a-ba86-35ff063e15d6"
      },
      "id": "JyXOcgbqrDyz",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello1\n",
            "hello2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lAz8NrLUgFnz"
      },
      "id": "lAz8NrLUgFnz",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}